# **Processed Files in LLaMA 2 Fine-Tuning Pipeline**

## **ğŸ“Œ Introduction**
In this presentation, we will explore the processed files generated in the LLaMA 2 fine-tuning pipeline. These files play a crucial role in training, evaluation, and deployment of the model.

---

## **ğŸ”¹ Overview of the Pipeline**
1. **Raw Data** (PDFs, Text, etc.) â†’ Extracted using `pdf_processing.py`
2. **Dataset Preparation** â†’ Processed into `dataset.txt`
3. **Tokenization** â†’ `tokenized_dataset/` created using `tokenize_dataset.py`
4. **Fine-Tuning** â†’ Model is trained and saved in `models/`
5. **Evaluation** â†’ Evaluated model performance with `evaluate.py`

---

## **ğŸ“‚ Detailed Explanation of Processed Files**

### **1ï¸âƒ£ `dataset.txt`**
- ğŸ“ **Location**: `data/processed/dataset.txt`
- ğŸ“„ **Content**: Cleaned and formatted text extracted from PDFs or other sources.
- ğŸ“Œ **Purpose**: Acts as the training dataset before tokenization.

âœ… **Example Content:**
```plaintext
"Dreams are a window into the subconscious."
"Freud's interpretation of dreams revolutionized psychology."
```

---

### **2ï¸âƒ£ `tokenized_dataset/`**
- ğŸ“ **Location**: `data/processed/tokenized_dataset/`
- ğŸ“„ **Content**: Hugging Face Dataset files (binary format) containing tokenized text.
- ğŸ“Œ **Purpose**: Stores text converted into token IDs, which are used for training the LLaMA 2 model.

âœ… **Files Inside `tokenized_dataset/`**
- `data-00000-of-00001.arrow` â†’ Stores tokenized text in Apache Arrow format.
- `dataset_info.json` â†’ Metadata about the dataset.
- `state.json` â†’ Tracks processing status.

âœ… **Example Tokenized Output:**
```json
{"text": "Dreams are a window into the subconscious.", "input_ids": [342, 654, 1234, ...], "attention_mask": [1, 1, 1, ...]}
```

---

### **3ï¸âƒ£ `models/` (Fine-Tuned Model Output)**
- ğŸ“ **Location**: `models/fine_tuned_llama2/`
- ğŸ“„ **Content**: Trained LLaMA 2 model weights and tokenizer.
- ğŸ“Œ **Purpose**: Stores the fine-tuned model for later use.

âœ… **Files Inside `models/`**
- `pytorch_model.bin` â†’ Model weights (large file!)
- `config.json` â†’ Model configuration
- `tokenizer.json` â†’ Tokenizer settings
- `special_tokens_map.json` â†’ Defines special tokens (e.g., `<PAD>`)

---

## **ğŸ“Š Data Flow in Fine-Tuning**
```plaintext
Raw PDFs â†’ dataset.txt â†’ tokenized_dataset/ â†’ Fine-Tuned Model (models/)
```

---

## **ğŸ›  Managing Processed Files in Git**

### **Why Arenâ€™t Processed Files Automatically Tracked?**
1. **They might be in `.gitignore`** â†’ Check & remove exclusions.
2. **Files are too large** â†’ Use `Git LFS` for large model weights.
3. **Some files are dynamically generated** â†’ Avoid pushing temporary files.

### **How to Track Processed Files?**
```bash
git add -f data/processed/dataset.txt
git add -f data/processed/tokenized_dataset/*
git add -f models/*
git commit -m "Adding processed files"
git push origin main
```

For large files:
```bash
git lfs track "models/*"
git add .gitattributes
git commit -m "Using Git LFS for large files"
git push origin main
```

---
