# **AI Dream Interpreter** ğŸŒ™ğŸ’­âœ¨  
**Fine-tune GPTâ€‘2 and T5 to interpret dreams based on structured text from PDFs.**

---

## ğŸŒŸ Overview  
This project fine-tunes two different language models â€“ **GPTâ€‘2** (a decoderâ€‘only model) and **T5** (an encoderâ€“decoder model) â€“ using dream interpretation texts extracted from PDFs. Both models are trained on structured dream data, and you can later compare their interpretation outputs to explore how different architectures affect the results.

### Pipeline  
- âœ… Extract text from PDFs ğŸ“„  
- âœ… Preprocess & tokenize the dataset ğŸ› ï¸  
- âœ… Fine-tune GPTâ€‘2 and T5 on dream-related data ğŸ§   
- âœ… Evaluate and compare model performance ğŸ“Š

---

## ğŸš€ Setup Guide (Python 3.10)

### 1ï¸âƒ£ Clone the Repository  
```sh
git clone https://github.com/Soncity2/NLP-project-dream-interpretation.git
cd NLP-project-dream-interpretation
```

### **2ï¸âƒ£ Create a Virtual Environment (Optional but Recommended)**
```sh
python3 -m venv venv_dream
source venv_dream/bin/activate  # On Windows use: venv\Scripts\activate
```
### **3ï¸âƒ£ Install Dependencies**
```sh
pip install -r requirements.txt
```
___
## ğŸ“‚ Project Structure
```
ai-dream-interpreter/
â”‚â”€â”€ config/                     
â”‚   â”œâ”€â”€ gpt2_training_config.yaml       # Training hyperparameters for GPT-2
â”‚   â”œâ”€â”€ t5_training_config.yaml       # Training hyperparameters for T5
â”‚â”€â”€ data/                         
â”‚   â”œâ”€â”€ raw_pdfs/                  # PDF files for fine-tuning
â”‚   â”œâ”€â”€ processed/                 # Extracted & processed text & CSVs
â”‚â”€â”€ models/                        # Trained models (e.g., fine_tuned_gpt2, fine_tuned_t5)
â”‚â”€â”€ scripts/                      
â”‚   â”œâ”€â”€ pdf_processing.py           # Extract text from PDFs
â”‚   â”œâ”€â”€ dataset_preparation.py      # Prepare dataset from processed text
â”‚   â”œâ”€â”€ tokenize_dataset.py         # Tokenize text dataset
â”‚   â”œâ”€â”€ fine_tune_gpt2.py           # Fine-tune GPT-2 on dream data
â”‚   â”œâ”€â”€ fine_tune_t5.py           # Fine-tune T5 on dream data
â”‚   â”œâ”€â”€ evaluate_models.py          # Evaluate and compare models
â”‚â”€â”€ logs/                          
â”‚â”€â”€ requirements.txt               
â”‚â”€â”€ setup.py                       
â”‚â”€â”€ README.md                      
â”‚â”€â”€ .gitignore
```
---
##  ğŸ¯ Usage Instructions

ğŸ“„ 1ï¸âƒ£ Extract Text from PDFs
Place your PDF files in data/raw_pdfs/ and run:

```sh
python scripts/pdf_processing.py
Output: Extracted text stored in data/processed/ as .txt files.
```


ğŸ›  2ï¸âƒ£ Prepare Dataset for Training
```sh
python scripts/dataset_preparation.py
Output: A structured dataset in data/processed/dataset.txt.
```
ğŸ”¢ 3ï¸âƒ£ Tokenize the Dataset
```sh
python scripts/tokenize_dataset.py
Output: Tokenized dataset stored in data/processed/tokenized_dataset/.
```
ğŸ“ 3ï¸âƒ£ Fine-Tune the Models on Dream Data
```sh
python scripts/fine_tune_gpt2.py
Output: Fine-tuned model saved in models/fine_tuned_gpt2/.
```
```sh
python scripts/fine_tune_t5.py
Output: Fine-tuned BART model is saved in models/fine_tuned_t5/.

```
ğŸ“Š 4ï¸âƒ£ Evaluate the Fine-Tuned Model
```sh
python scripts/evaluate_models.py
Output: Evaluation results saved in data/processed/evaluation_results.json.
```
___
### ğŸ›  Customization & Configuration
Modify hyperparameters in config/gpt2_training_config.yaml before fine-tuning.
and config/t5_training_config.yaml

Example:
```
num_train_epochs: 5
learning_rate: 3e-5
batch_size: 8
```
___
### ğŸ“Œ Future Improvements

ğŸ”¹ Use Reinforcement Learning to refine dream interpretations. 

ğŸ”¹ Add a web-based UI for interactive dream analysis.

ğŸ”¹ Enable multilingual dream analysis (e.g., Spanish, French).
___
### â¤ï¸ Contributions & Support
Want to improve AI dream interpretation? Feel free to contribute or open an issue! ğŸš€
Happy Dreaming! âœ¨ğŸŒ™ğŸ’¬
